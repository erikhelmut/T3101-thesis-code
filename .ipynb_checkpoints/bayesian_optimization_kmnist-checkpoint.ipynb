{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimization\n",
    "\n",
    "A Bayesian optmization example for tuning the learning rate in training a ResNet9 on the KMNIST dataset. As a surrogate model a Gaussian Process is used, while as acquisition we use expected improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from typing import Union, Callable\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#import seaborn as sns\n",
    "#sns.set()\n",
    "\n",
    "import sklearn.gaussian_process as gp\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Network Structure\n",
    "\n",
    "A small <a href=\"https://arxiv.org/abs/1512.03385\">ResNet9</a> is chosen, as this does not have too many parameters to be optimized in a reasonable amount of time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(in_channels: int, out_channels: int, pool: bool = False, pool_kernel_size: int = 2):\n",
    "    '''\n",
    "    Convolutional building block of ResNet\n",
    "    \n",
    "    Args:\n",
    "        in_channels: channels of input tensor\n",
    "        out_channels: channels of out put tensor\n",
    "        pool: wether to apply pooling after conv\n",
    "        pool_kernel_size: kernel size of pooling operation\n",
    "    \n",
    "    Returns:\n",
    "        Conv Blokc for ResNet\n",
    "    '''\n",
    "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), \n",
    "              nn.BatchNorm2d(out_channels), \n",
    "              nn.ReLU(inplace=True)]\n",
    "    if pool: layers.append(nn.MaxPool2d(pool_kernel_size))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class ResNet9(nn.Module):\n",
    "    '''\n",
    "    Small ResNet9 for Classification\n",
    "    \n",
    "    Args:\n",
    "        in_channels: channels of input image\n",
    "        num_classes: number of classes in dataset\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, in_channels: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = conv_block(in_channels, 64)\n",
    "        self.conv2 = conv_block(64, 128, pool=True, pool_kernel_size=3)\n",
    "        self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128))\n",
    "        \n",
    "        self.conv3 = conv_block(128, 256, pool=True)\n",
    "        self.conv4 = conv_block(256, 512, pool=True, pool_kernel_size=3)\n",
    "        self.res2 = nn.Sequential(conv_block(512, 512), conv_block(512, 512))\n",
    "\n",
    "        self.classifier = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv1(x)\n",
    "        import pdb;pdb.set_trace()\n",
    "        x = self.conv2(x)\n",
    "        x = self.res1(x) + x\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.res2(x) + x\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Helper Functions for Neural Network's Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(batch_size: int = 100):\n",
    "    '''\n",
    "    Helper fct. for loading KMNIST datasets\n",
    "    \n",
    "    Args:\n",
    "        batch_size: Batch size for training and testing\n",
    "    \n",
    "    Returns:\n",
    "        train_loader: Training dataset\n",
    "        test_loader: Testing dataset\n",
    "    '''\n",
    "    \n",
    "    #subset_indices = torch.arange(0, 1000, 1)\n",
    "    trainset = torchvision.datasets.KMNIST(root='./data', train=True,\n",
    "                                            download=True, transform=transforms.ToTensor())\n",
    "    train_loader = DataLoader(trainset, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, num_workers=2) \n",
    "                              #num_workers=2, sampler=torch.utils.data.sampler.SubsetRandomSampler(subset_indices))\n",
    "\n",
    "    testset = torchvision.datasets.KMNIST(root='./data', train=False,\n",
    "                                           download=True, transform=transforms.ToTensor())\n",
    "    test_loader = DataLoader(testset, batch_size=BATCH_SIZE,\n",
    "                             shuffle=False, num_workers=2)\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def weight_reset(m: nn.Module):\n",
    "    '''\n",
    "    Helper fct. for resetting the weights of a neural network.\n",
    "    '''\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        m.reset_parameters()\n",
    "        \n",
    "\n",
    "def accuracy(outputs: torch.Tensor, labels: torch.Tensor) -> float:\n",
    "    '''\n",
    "    Helper fct. for calculating the accuracy of network's prediction\n",
    "    \n",
    "    Args:\n",
    "        outputs: probabilistic outputs of net\n",
    "        labels: ground truth labels\n",
    "    \n",
    "    Returns:\n",
    "        accuracy: prob accuracy\n",
    "    '''\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "\n",
    "def train_epoch(data_loader: DataLoader, net: nn.Module, criterion: nn.Module,\n",
    "               optimizer: optim.Optimizer) -> (float):\n",
    "    '''\n",
    "    Helper fct. for training net for one epoch.\n",
    "    \n",
    "    Args:\n",
    "        data_loader: datset as PyTorch DataLoader object\n",
    "        net: neural network\n",
    "        criterion: loss function\n",
    "        optimizer: Optimizer for net\n",
    "    \n",
    "    Returns:\n",
    "        train_loss: average loss during training epoch\n",
    "        train_acc: average accuracy during training epoch\n",
    "    '''\n",
    "    \n",
    "    net.train()\n",
    "    \n",
    "    train_loss, train_acc = 0, 0\n",
    "    pbar = tqdm(data_loader, file=sys.stdout)\n",
    "    \n",
    "    for images, labels in pbar:\n",
    "        \n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * images.shape[0]\n",
    "        \n",
    "        acc = 100 * accuracy(F.softmax(outputs), labels)\n",
    "        train_acc += acc\n",
    "        \n",
    "        pbar.set_description(f'Train acc: {acc:.2f}\\tTrain loss: {loss.item():.3f}')\n",
    "    \n",
    "    train_acc = train_acc / len(data_loader.dataset)\n",
    "    train_loss = train_loss / len(data_loader.dataset)\n",
    "    \n",
    "    return train_acc, train_loss\n",
    "\n",
    "\n",
    "def validate(data_loader: DataLoader, net: nn.Module, criterion: nn.Module) -> (float):\n",
    "    '''\n",
    "    Helper fct. for validation/testing of net.\n",
    "    \n",
    "    Args:\n",
    "        data_loader: datset as PyTorch DataLoader object\n",
    "        net: neural network\n",
    "        criterion: loss function\n",
    "    \n",
    "    Returns:\n",
    "        test_loss: average loss during testing epoch\n",
    "        test_acc: average accuracy during testing epoch\n",
    "    '''\n",
    "    \n",
    "    net.eval()\n",
    "    \n",
    "    test_loss, test_acc = 0, 0\n",
    "    pbar = tqdm(data_loader, file=sys.stdout)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for images, labels in pbar:\n",
    "            \n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item() * images.shape[0] \n",
    "            \n",
    "            acc = 100 * accuracy(F.softmax(outputs), labels)\n",
    "            test_acc += acc\n",
    "            \n",
    "            pbar.set_description(f'Test acc: {acc:.2f}\\tTest loss: {loss.item():.3f}')\n",
    "            \n",
    "    test_loss = test_loss / len(data_loader.dataset)\n",
    "    test_acc = test_acc / len(data_loader)\n",
    "    \n",
    "    return test_acc, test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Objective Function from which to Sample\n",
    "\n",
    "We choose the test accuracy as our obtimization fct. It is desirable to have an as high as possible accuracy during testing. Each sampling of test accuray involves training the neural network, which is quite expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_acc(params: [float], data_loaders: [DataLoader], net: nn.Module, \n",
    "               criterion: nn.Module, num_epochs: int = 1) -> np.ndarray:\n",
    "    '''\n",
    "    Objective fct. for Bayesian optimization\n",
    "    \n",
    "    Args:\n",
    "        params: the parameters which shall be optimized\n",
    "        data_loaders: training and testing data loader\n",
    "        net: neural network\n",
    "        criterion: loss fct. for training\n",
    "        num_epochs: number of epochs to train the network in ech BO optimization step\n",
    "    \n",
    "    Returns:\n",
    "        test_acc: Average test accuracy after training num_epochs\n",
    "    '''\n",
    "\n",
    "    params = params.flatten()\n",
    "    net.apply(weight_reset)\n",
    "\n",
    "    optimizer = optim.AdamW(net.parameters(), lr=params[0])#lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    \n",
    "    train_losses, train_accs, test_losses, test_accs = [], [], [], []\n",
    "\n",
    "    for e in range(1, num_epochs + 1):\n",
    "\n",
    "        train_acc, train_loss = train_epoch(data_loaders[0], net, criterion, optimizer)\n",
    "\n",
    "        train_accs.append(train_acc)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        test_acc, test_loss = validate(data_loaders[1], net, criterion)\n",
    "\n",
    "        test_accs.append(test_acc)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        print(f\"Epoch: {e}\\tTrain Loss: {train_loss:.3f}\\tTrain Acc: {train_acc:.2f}\\t\\\n",
    "                Test Loss: {test_loss:.3f}\\tTest Acc: {test_acc:.2f}\")\n",
    "        \n",
    "    return np.array([[test_acc.item()]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Bayesian Optimization Class\n",
    "\n",
    "We use a Gaussian Process as predictive (surrogate) model for Bayesian Optimization (BO). For an introduction to Gaussian Processes you can go to <a href=\"https://gist.github.com/maltetoelle/282144771f2fe1d39c0f14b5356e78e4\">gist.github.com/maltetoelle</a>. As acquistion function \"Expected Improvement\" is used. The model can easily be extended to other acquisition functions.\n",
    "\n",
    "Proposing samples in the search space is done by the acquisition functions, that must be trade of between exploration and exploitation. Both correspond to high values of the acquisition function, which is maximized to determine the next sampling point. Formally the objective function (our sampling function from above) $f$ will be sampled at\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_t = \\mathrm{arg\\,max}_{\\mathbf{x}} u(\\mathbf{x}|\\mathcal{D}_{1:t-1})\n",
    "\\tag{1}\n",
    "$$\n",
    "\n",
    "where $u$ is our acquisition function and $\\mathcal{D}_{1:t-1}=\\{(\\mathbf{x}_1,y_1,\\ldots,\\mathbf{x}_2,y_2)\\}$ are the $t-1$ samples drawn from $f$ so far. The optimization algorithm can be summarized as follows so far\n",
    "\n",
    "For $t=1,2,\\ldots$ repeat:\n",
    "- Find next sampling point $\\mathbf{x}_t$ by optimizing the acquisition function over our surrogate model, the GP, according to Eq. 1\n",
    "- Obtain a sample $y$ from the objective function $f$ at $\\mathbf{x}_t$\n",
    "- Add sample to previous samples $\\mathcal{D}_{1:t}$ and update GP\n",
    "\n",
    "Now the question remains, which acquistiotn function to use. In the following we will us \"Expected Improvement\", as it is the most widely used one. Expected improvement is defined as\n",
    "\n",
    "$$\n",
    "\\mathrm{EI}(\\mathbf{x}) = \\mathbb{E}\\max \\left(f(\\mathbf{x})-f(\\mathbf{x}^+),0\\right) ~,\n",
    "$$\n",
    "\n",
    "where $f(\\mathbf{x}^+)$ is the value of the best sample so far. The expected improvement can be evaluated with a GP model as\n",
    "\n",
    "$$\n",
    "\\mathrm{EI}(\\mathbf{x}) = \\left\\{\n",
    "    \\begin{array}{ll}\n",
    "        \\left( \\mu(\\mathbf{x}) - f(\\mathbf{x}^+) - \\xi \\right) \\boldsymbol{\\Phi}(Z) + \\sigma(\\mathbf{x})\\Phi(Z) & \\mathrm{if}\\; \\sigma(\\mathbf{x}) > 0 \\\\\n",
    "        0 & \\mathrm{if}\\; \\sigma(\\mathbf{x}) = 0\n",
    "    \\end{array}\n",
    "\\right. ~,\n",
    "\\tag{2}\n",
    "$$\n",
    "\n",
    "with\n",
    "\n",
    "$$\n",
    "Z = \\left\\{\n",
    "    \\begin{array}{ll}\n",
    "        \\frac{\\mu(\\mathbf{x} - f(\\mathbf{x}^+) - \\xi}{\\sigma(\\mathbf{x})} & \\mathrm{if}\\; \\sigma(\\mathbf{x}) > 0 \\\\\n",
    "        0 & \\mathrm{if}\\; \\sigma(\\mathbf{x}) = 0\n",
    "    \\end{array}\n",
    "\\right. ~,\n",
    "$$\n",
    "\n",
    "where $\\mu(\\mathbf{x})$ and $\\sigma(\\mathbf{x})$ are the mean and standard deviation of our GP posterior at location $\\mathbf{x}$. $\\boldsymbol{\\Phi}$ and $\\Phi$ denote the CDF and PDF respectively of the Gaussian distribution. The first term in Eq. 2 determines the magnitude of exploitation, whereas the second controls the amount of exploration. Higher values of $\\xi$ lead to more exploration. Now we have the minimum at hand to code an example for Bayesian optimization with a Gaussian process as surrogate model and expected improvement as acquisition function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianOptimization:\n",
    "    '''\n",
    "    Class for performing Bayesian optimization. \n",
    "    For now only \"expected improvement\" is possible as acquisition fct. \n",
    "    But the code can easily extended to other fct.'s\n",
    "    \n",
    "    Args:\n",
    "        kernel: kernel for GP regression (is multiplied with constant kernel)\n",
    "        kernel_params: kernel parameters such as lengthscale in format {'length_scale': 0.01}\n",
    "        bounds: bounds for the different parameters that shall be optimized\n",
    "        obj_fct: objective function that sample the value of the function which to optimize\n",
    "        acquisition: Either str for using an already implemented fct.\n",
    "                     Or self defined fct. (Callable)\n",
    "        n_init: number of initial points to sample from objective function\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, kernel: gp.kernels.Kernel, kernel_params: dict, bounds: np.ndarray, \n",
    "                obj_fct: Callable, acquisition: Union[Callable, str] = 'expected_improvement', n_init: int = 1):\n",
    "        \n",
    "        # multiply with constant kernel\n",
    "        kernel = gp.kernels.ConstantKernel(1.0) * kernel(**kernel_params)\n",
    "        self.gpr = gp.GaussianProcessRegressor(kernel=kernel)\n",
    "        \n",
    "        self.obj_fct = obj_fct\n",
    "        \n",
    "        # initial parameter sample\n",
    "        self.params_samples = np.random.uniform(bounds[:,0], bounds[:,1], size=(bounds.shape[0], 1))\n",
    "        self.next_params = None\n",
    "        self.cost_samples = self.obj_fct(self.params_samples)\n",
    "        \n",
    "        # if greater than zero sample more initial points\n",
    "        if n_init > 1:\n",
    "            params_sample = np.random.uniform(bounds[:,0], bounds[:,1], size=(bounds.shape[0], 1))\n",
    "            cost_sample = obj_fct(params_sample)\n",
    "            \n",
    "            self.params_samples = np.vstack((self.params_samples, params_sample))\n",
    "            self.cost_samples = np.vstack((self.cost_samples, cost_sample))\n",
    "        \n",
    "        self.n_init = n_init\n",
    "        self.dim = self.params_samples.shape[1]\n",
    "        self.bounds = bounds\n",
    "        \n",
    "        # initialize whole parameter space for GP regression\n",
    "        self.params_space = np.linspace(bounds[:,0], bounds[:,1], 100)\n",
    "        \n",
    "        # other acquisition functions can be added here\n",
    "        if type(acquisition) == str:\n",
    "            if acquisition == 'expected_improvement': self.acquisition = self.expected_improvement\n",
    "\n",
    "    def propose_location(self, n_restarts: int = 25) -> np.ndarray:\n",
    "        '''\n",
    "        Proposes the next sampling point by optimizing the acquisition fct.\n",
    "\n",
    "        Args:\n",
    "            n_restars: restarts for minimizer to find max of acquisition fct.\n",
    "\n",
    "        Returns:\n",
    "            Location of the acquisition function maximum\n",
    "        '''\n",
    "\n",
    "        min_val = 1\n",
    "        min_x = None\n",
    "\n",
    "        def min_obj(params):\n",
    "            # Minimization objective is the negative acquisition function\n",
    "            return -self.acquisition(self.gpr, params.reshape(-1, self.dim), self.params_samples[-1], self.cost_samples[-1])\n",
    "\n",
    "        # Find the best optimum by starting from n_restart different random points.\n",
    "        for x0 in np.random.uniform(self.bounds[:, 0], self.bounds[:, 1], size=(n_restarts, self.dim)):\n",
    "            res = minimize(min_obj, x0=x0, bounds=self.bounds, method='L-BFGS-B')        \n",
    "            if res.fun < min_val:\n",
    "                min_val = res.fun[0]\n",
    "                min_x = res.x  \n",
    "\n",
    "        return min_x.reshape(-1, 1)\n",
    "    \n",
    "    def optimize(self, n_iter: int = 10, n_restarts: int = 25, plot: bool = True) -> (np.ndarray):\n",
    "        '''\n",
    "        Fct. for performing Bayesian optimization.\n",
    "        \n",
    "        Args:\n",
    "            n_iter: optimization budget i.e. how many optimizations to perform\n",
    "            n_restarts: restarts for minimizer to find max of acquisition fct.\n",
    "            plot: wether to plot results during training\n",
    "        \n",
    "        Returns:\n",
    "            Best parameters\n",
    "        '''\n",
    "\n",
    "        for i in range(n_iter):\n",
    "\n",
    "            # Update Gaussian process with existing samples\n",
    "            self.gpr.fit(self.params_samples, self.cost_samples)\n",
    "\n",
    "            # Obtain next sampling point from the acquisition fct.\n",
    "            self.next_params = self.propose_location(n_restarts)\n",
    "\n",
    "            # Obtain next noisy sample from the objective fct.\n",
    "            next_cost = self.obj_fct(self.next_params)\n",
    "            \n",
    "            if plot:\n",
    "                self.plot_optimization()\n",
    "            \n",
    "            # Add sample to previous samples\n",
    "            self.params_samples = np.vstack((self.params_samples, self.next_params))\n",
    "            self.cost_samples = np.vstack((self.cost_samples, next_cost))\n",
    "        \n",
    "        return self.params_samples[np.argmin(self.cost_samples)]\n",
    "    \n",
    "    @staticmethod\n",
    "    def expected_improvement(gpr: gp.GaussianProcessRegressor, params_space: np.ndarray,\n",
    "                             cost_samples: np.ndarray, xi: float = 0.01) -> np.ndarray:\n",
    "        '''\n",
    "        Computes the EI at points for the parameter space based on \n",
    "        cost samples using a Gaussian process surrogate model.\n",
    "\n",
    "        Args:\n",
    "            gpr: A GaussianProcessRegressor fitted to samples.\n",
    "            params_space: Parameter space at which EI shall be computed (m x d).\n",
    "            cost_samples: Sample values (n x 1).\n",
    "            xi: Exploitation-exploration trade-off parameter.\n",
    "\n",
    "        Returns:\n",
    "            Expected improvements for paramter space.\n",
    "        '''\n",
    "        # make prediction for whole parameter space\n",
    "        mu, sigma = gpr.predict(params_space, return_std=True)\n",
    "\n",
    "        sigma = sigma.reshape(-1, 1)\n",
    "        \n",
    "        # calculate ei according to Eq. 2\n",
    "        # But we have to make sure to not devide by 0\n",
    "        with np.errstate(divide='warn'):\n",
    "            imp = mu - np.max(cost_samples) - xi\n",
    "            Z = imp / sigma\n",
    "            ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "            ei[sigma == 0.0] = 0.0\n",
    "\n",
    "        return ei\n",
    "    \n",
    "    def plot_optimization(self):\n",
    "        '''\n",
    "        Helper fct. for plotting results during training.\n",
    "        '''\n",
    "        \n",
    "        cp = sns.color_palette()\n",
    "        \n",
    "        mu, sigma = self.gpr.predict(self.params_space, return_std=True)\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        for i in range(1, 3):\n",
    "            ax.fill_between(self.params_space.ravel(),\n",
    "                            mu.ravel() + i * sigma,\n",
    "                            mu.ravel() - i * sigma,\n",
    "                            color=cp[0],\n",
    "                            alpha=0.1*i)\n",
    "        sur_fct, = ax.plot(self.params_space, mu, c=cp[1], label='Surrogate fct')\n",
    "        cost_samples, = ax.plot(self.params_samples, self.cost_samples, 'kx', mew=3, label='Cost samples')\n",
    "        \n",
    "        ax2 = ax.twinx()\n",
    "        acquisition = self.acquisition(self.gpr, self.params_space, self.cost_samples)\n",
    "        acq_fct, = ax2.plot(self.params_space, \n",
    "                            acquisition,\n",
    "                            color=cp[2],\n",
    "                            label='Acquisition fct', zorder=0)\n",
    "        ax2.fill_between(self.params_space.ravel(), acquisition.ravel(), 0, color=cp[2], zorder=0)\n",
    "        ax2.set_ylim([0, ax2.get_ylim()[1]*4])\n",
    "        \n",
    "        \n",
    "        if self.next_params:\n",
    "            ax.axvline(x=self.next_params, ls='--', c='r', zorder=10)\n",
    "        \n",
    "        ax.legend(handles=[sur_fct, cost_samples, acq_fct], loc='upper right')\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    def plot_convergence(self):\n",
    "        '''\n",
    "        Helper fct. to plot convergence after training.\n",
    "        '''\n",
    "        \n",
    "        x = self.params_samples[self.n_init:].ravel()\n",
    "        y = self.cost_samples[self.n_init:].ravel()\n",
    "        r = range(1, len(x)+1)\n",
    "\n",
    "        x_neighbor_dist = [np.abs(a-b) for a, b in zip(x, x[1:])]\n",
    "        y_max_watermark = np.maximum.accumulate(y)\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,5))\n",
    "        ax1.plot(r[1:], x_neighbor_dist, 'bo-')\n",
    "        ax1.set_xlabel('Iteration')\n",
    "        ax1.set_ylabel('Distance')\n",
    "        ax1.set_title('Distance between consecutive params\\'s')\n",
    "\n",
    "        ax2.plot(r, y_max_watermark, 'ro-')\n",
    "        ax2.set_xlabel('Iteration')\n",
    "        ax2.set_ylabel('Best params')\n",
    "        ax2.set_title('Value of best selected sample')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Hyperparameters of Neural Network's Training\n",
    "\n",
    "These parameters can obviously be optimization with BO as well. Just the visualization fct. must be adjusted for 2D data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 1\n",
    "WEIGHT_DECAY = 1e-4\n",
    "BATCH_SIZE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Bayesian Optimization\n",
    "\n",
    "As kernel we use the Matern kernel as generalization of the squared exponential kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-images-idx3-ubyte.gz to ./data\\KMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\KMNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\KMNIST\\raw\n",
      "Downloading http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-labels-idx1-ubyte.gz to ./data\\KMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "111.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\KMNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\KMNIST\\raw\n",
      "Downloading http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-images-idx3-ubyte.gz to ./data\\KMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.2%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\KMNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\KMNIST\\raw\n",
      "Downloading http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-labels-idx1-ubyte.gz to ./data\\KMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "160.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\KMNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\KMNIST\\raw\n",
      "Processing...\n",
      "Done!\n",
      "  0%|          | 0/600 [00:00<?, ?it/s]> \u001b[1;32m<ipython-input-4-7d34e49aea31>\u001b[0m(45)\u001b[0;36mforward\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m     43 \u001b[1;33m        \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     44 \u001b[1;33m        \u001b[1;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[1;33m;\u001b[0m\u001b[0mpdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m---> 45 \u001b[1;33m        \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     46 \u001b[1;33m        \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mres1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     47 \u001b[1;33m        \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# intilialie neural network and the trainig necessities\n",
    "data_loaders = load_data(batch_size=BATCH_SIZE)\n",
    "net = ResNet9(in_channels=1, num_classes=10).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# make sure the objective function only needs the parameter as argument\n",
    "obj_fct = lambda params: sample_acc(params, data_loaders, net, criterion, NUM_EPOCHS)\n",
    "\n",
    "# bounds for learning rate between which to optimize\n",
    "bounds = np.array([[1e-4, 5e-2]])\n",
    "\n",
    "# define kernel for GP regression\n",
    "# Matern kernel as generalization of squared exponential is chosen\n",
    "kernel = gp.kernels.Matern\n",
    "kernel_params = {'length_scale': 3e-3, 'nu': 1.5}\n",
    "\n",
    "# Initilize BO class with \"expected improvement\" as acquistiotn fct.\n",
    "bayesian_optimization = BayesianOptimization(kernel=kernel, kernel_params=kernel_params, bounds=bounds, \n",
    "                                             obj_fct=obj_fct, acquisition='expected_improvement', n_init=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_budget = 10\n",
    "best_param = bayesian_optimization.optimize(n_iter=optimization_budget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesian_optimization.plot_convergence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
